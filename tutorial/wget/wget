<?xml version="1.0" encoding="iso-8859-1"?>

<chapter>

 <title>wget</title>

 <author>
  <name>Johnny Graber</name>
  <mailto>linux@jgraber.ch</mailto>
 </author>

 <layout>
  <name>Torsten Hemm</name>
  <mailto>T.Hemm@gmx.de</mailto>
 </layout>

 <license>GFDL</license>

 <index>wget</index>

 <description> 
  <textblock>
GNU <command>wget</command> ist ein praktisches Tool, um Dateien aus dem Web zu 
laden. Über zahlreiche Optionen kann man genau das 
erreichen. Sogar abgebrochene Downloads können
wieder aufgenommen werden. Es werden Dateien und Seiten sowohl
per http, als auch per ftp geladen werden.
  </textblock>
 </description>

 <split>
  <section>
   <heading>
Der erste Einsatz von wget
   </heading>
 
   <textblock>
<command>wget</command> wird mit jeder halbwegs aktuellen Distribution
mitgeliefert. Sollte es tatsächlich nicht installiert sein, findet man 
es auf <ref lang="en" url="http://www.gnu.org/software/wget">http://www.gnu.org/software/wget/</ref> 
   </textblock>
  
   <textblock>
Der Aufruf von <command>wget</command> lautet
   </textblock>
  
   <shell>
    <user path="~">
wget [Optionen] URL
    </user>
   </shell>

   <textblock>
Will man sich das Basisrelease von Selflinux besorgen, lautet 
der Aufruf
   </textblock>

   <shell>
    <user path="~">
wget http://mesh.dl.sourceforge.net/sourceforge/selflinux/SelfLinux-0.10.0-html.tar.gz
    </user>
   </shell>

   <textblock>
Sollte der Download ausirgend einem Grund abbrechen, kann er 
mit der Option <command>-c</command> wieder aufgenommen werden:
   </textblock>

   <shell>
    <user path="~">
wget -c http://mesh.dl.sourceforge.net/sourceforge/selflinux/SelfLinux-0.10.0-html.tar.gz
    </user>
    <output>
--21:41:52--  http://mesh.dl.sourceforge.net/sourceforge/selflinux/SelfLinux-0.10.0-html.tar.gz
          => `SelfLinux-0.10.0-html.tar.gz'
Auflösen des Hostnamen »mesh.dl.sourceforge.net«.... 213.203.218.122
Verbindungsaufbau zu mesh.dl.sourceforge.net[213.203.218.122]:80... verbunden.
HTTP Anforderung gesendet, warte auf Antwort... 206 Partial Content
Länge: 4,039,004 (noch 3,963,516) [application/x-tar]

100%[+++==========================================================>] 4,039,004    112.39K/s    ETA 00:00

21:42:25 (120.79 KB/s) - »SelfLinux-0.10.0-html.tar.gz« gespeichert [4039004/4039004]
    </output>
   </shell>

   <textblock>
<command>wget</command> zeigt einem alle wichtigen Optionen auf einen 
Blick an. Der Statusbalken zeigt, wie weit man schon vorangeschritten ist, 
danach folgt die Angabe der aktuellen Geschwindigkeit und hinter 
ETA steht die verbleibende Zeit. 
   </textblock>

   <textblock>
<strong>+</strong>-Zeichen im Statusbalken zeigen Dateifragmente, die durch Wiederaufnahme 
des Downloads fortgesetzt wurden.
   </textblock>

   <textblock>
Die Option <command>-c</command> ist gerade bei großen Dateien wie
ISO-Images sehr angenehm. Bricht der Download über einen Webbrowser
bei 600 von 650 MB ab, ist die Datei verloren. Mit <command>wget</command>
genügt das <command>-c</command> und es werden nur noch die fehlenden Teile
heruntergeladen.
   </textblock>
  </section> 
 </split>

 <split>
  <section>
	 
   <heading>		
Spiegeln von Webseiten
   </heading>
 
   <textblock>
Mit <command>wget</command> können nicht nur einzelne Dateien gespeichert, 
sondern auch ganze Seiten gespiegelt werden. Die dafür zu verwendende 
Option ist <command>-r</command>. Damit wird bei der angegebenen Seite 
rekursiv den Links gefolgt. 
Die Standardeinstellung für <command>-r</command> folgt fünf Ebenen von
Links. Dabei wird nicht unterschieden, ob die Seiten vom gleichen Server stammen,
oder nicht. 
   </textblock>

   <textblock>
Bei FTP-Adressen werden demensprechend fünf Unterverzeichnisebenen heruntergeladen.
   </textblock>
  
   <textblock>
Ebenen heißen an dieser Stelle einfach verschachtelte Links.
Man muss sich dies wie eine gewöhnliche Sitzung mit einem Browser
vorstellen. Jeder Link, den man anklickt, öffnet eine neue Ebene.
Ein <command>-r</command> bedeutet also, daß man von der Startseite
fünf tiefer gelegene Seiten aufrufen kann.
Fünf Ebenen sind eine ganze Menge, da die Link-Anzahl der gefolgt wird, in dieser 
Einstellung nicht begrenzt ist. Wenn z.B. die erste Seite 20 Links enthält, diese 20
jeweils weitere 10, dann werden bereit 1 + 20 + 10 * 20 = 121 Seiten heruntergeladen.
Das wären also drei Ebenen.
   </textblock>

   <textblock>
Weist ein Link auf der 2. Seite zu
<ref lang="de" url="http://www.linux.org">www.linux.org</ref>, wird auch dort
wiederum den Links gefolgt und die Dateien auf dem eigenen 
Rechner abgelegt. Dabei sollte man immer bedenken, daß alle verlinkten Dokumente
geladen werden und das können schnell mehrere hundert Megabyte sein.
   </textblock>

   <textblock>
Die Option <command>-l num</command> steht für <strong>level</strong> und passt 
die Tiefe von <command>-r</command> an. <command>num</command> muss durch eine beliebige
positive Zahl ersetzt werden. 
   </textblock>

   <shell>
    <user path="~">
wget -r -l 2 www.selflinux.org
    </user>
   </shell>
  
   <textblock>
Speichert alle Dateien, die über eine andere Datei verlinkt sind,
im Verzeichnis <path>www.selflinux.org</path>
Die gefundene Verzeichnisstruktur wird dabei übernommen. Allerdings wird 
nur 2 Ebenen tief gesucht, was bei großen Kapiteln dazu führt, daß
nicht alle Dateien heruntergeladen werden.
   </textblock>

   <textblock>
<command>wget</command> ist gut um sich schnell einige Seiten zu holen. Für ein 
effektives Spiegeln eines Servers sollte man sich ein anderes Tool suchen. 
   </textblock>
  </section>
 </split>

 <split>
  <section>
   <heading>
Das Verhalten von wget anpassen
   </heading>
   
   <section>
    <heading>  
Unterdrücken und Erzwingen von Ordnern
    </heading>
    
    <textblock>
Bei dem Aufruf von <command>wget -r</command> wird immer ein Ordner mit
dem Namen der Webseite erstellt. Will man dies verhindern, lautet der 
Aufruf <command>wget -r -nd</command>.
Aber Vorsicht mit gleich lautenden Dateinamen: Sollte ein Name
schon vorhanden sein, überschreibt <command>wget</command> den Inhalt 
ohne zu fragen.
    </textblock>

    <textblock>
Will man das Anlegen der Ordner aus irgendeinem Grund erzwingen,
lautet die Option <command>-x</command> oder in der langen Version 
<command>--force-directories</command>.
Die Verzeichnisstruktur wird nun komplett übernommen.
    </textblock>

    <textblock>
Neben diesen beiden bietet <command>wget</command> noch eine dritte 
Möglichkeit. Hierbei wird die Verzeichnisstruktur übernommen, doch 
wird der Ordner mit dem Domainnamen weggelassen. Dies erreicht man 
mit <command>-nH</command> (<command>--no-host-directories</command>).
    </textblock>
   </section>  

   <section>
    <heading>
Nur bestimmte Dateitypen herunterladen
    </heading>
    
    <textblock> 
Wildcards können bei <command>wget</command> nicht verwendet werden, wenn die
Dateien per <strong>http</strong> geladen werden.
Es gibt aber dennoch eine Möglichkeit, nur spezielle Dateitypen zu bekommen.
Dafür muss man eine Liste mit <command>-A</command> (<command>--accept</command>) erstellen.
FTP kennt wildcards!
    </textblock>

    <shell>
     <user path="~">
wget -r -A jpg,png http://www.selflinux.org
     </user>
    </shell>

    <textblock>
Bei diesem Aufruf werden rekursiv die Dokumente nach <strong>*.jpg</strong> und 
<strong>*.png</strong> durchsucht und abgespeichert. Da HTTP keinen List-Befehl
kennt, muss <command>wget</command> zuerst alle HTML-Dateien herunterladen,
um an die Links zu kommen. Sobald die Bilder gefunden sind, werden 
die HTML-Dateien gelöscht.
    </textblock>	

    <textblock>
Der Umkehrbefehl von <command>-A</command> ist <command>-R (--reject)</command>.
Sollen alle Dateien, ausser <strong>*.jpg</strong> und <strong>*.png</strong> geholt
werden, lautet der Aufruf
    </textblock>
     
    <shell>
     <user path="~">   
wget -r -R jpg,png http://www.selflinux.org
     </user>
    </shell>
   </section>

   <section>
    <heading>
Grössenbegrenzung des Downloads
    </heading>

    <textblock>
Auf die Größe des Downloads kann aber nicht nur über <command>-A
</command> und <command>-R</command> Einfluss genommen werden, sondern
auch mittels <command>-Q</command> (<command>--quota</command>). Die Größenangabe erfolgt
in Bytes und legt den Wert für den ganzen Download fest. 
Da die Angabe großer Werte in Bytes mühsam ist, kann man auch 
andere Einheiten verwenden. Für <strong>Megabytes</strong> wird an die Zahl ein <strong>m</strong>
angehängt, für <strong>Kilobytes</strong> dient ein <strong>k</strong>.
    </textblock>
     
    <shell>
     <user path="~">
wget -r -nH -Q5m http://www.selflinux.org
     </user>
    </shell>
     
    <textblock>
Damit werden maximal 5 Megabyte Daten von <ref lang="de" url="htpp://www.selflinux.org">
www.selflinux.org</ref> geholt und im aktuellen Verzeichnis abgelegt.
Sind weniger als 5 MB Daten vorhanden, kann <command>wget</command> ja nicht
das ganze Quota ausnutzen.
    </textblock>
   </section>

   <section>
    <heading>
Dateien vor Download auf Datum prüfen
    </heading>
     
    <textblock>    
Holt man sich öfters Daten vom gleichen Server, möchte man ja nur
die neuesten Dateien herunterladen. Mit <command>-N</command> (<command>--timestamping</command>)
veranlasst man <command>wget</command>, vor dem Download das Datum der Datei auf dem
Server mit dem der lokalen Kopie zu vergleichen. Nur wenn die 
lokale Datei veraltet ist, beginnt <command>wget</command> mit dem Download.
    </textblock>

    <shell>
     <user path="~">
wget -N http://www.selflinux.org
     </user>
    </shell> 
   </section>

   <section>
    <heading>
Verwenden eines Proxy-Servers
    </heading>

    <textblock>
Will man einen Proxy-Server verwenden, genügt die Option 
<command>-Y on/off</command>. Dabei wird auf die Umgebungsvariablen
<path>$http_proxy</path> und <path>$FTP_PROXY</path> ausgelesen. Diese müssen
natürlich gesetzt werden:
    </textblock>

    <shell>
     <user path="~"> 
export http_proxy="http://meinproxy.provider.de:3128/"
     </user> 
     <user path="~">
export FTP_PROXY="http://meinproxy.provider.de:3128/"
     </user>
    </shell> 
   </section>
  </section> 
 </split>

 <split>
  <section>
   <heading>
Die Dateien /etc/wgetrc und .wgetrc
   </heading>
   
   <textblock>
Eine große Anzahl der Startoptionen können in diese 
Konfigurationsdateien eingetragen werden. Die Datei 
<path>/etc/wgetrc</path> gilt für alle User, die 
<path>~/.wgetrc</path> nur für den jeweiligen Benutzer.
   </textblock>
    
   <textblock>
Hier ein kleines Beispiel des Aufbaus einer solchen Datei:
   </textblock>

   <file>
    <title>
.wgetrc
    </title>
    <content>
      <![CDATA[
###
### Sample Wget initialization file .wgetrc
###

# You can set retrieve quota for beginners by specifying a value
# optionally followed by 'K' (kilobytes) or 'M' (megabytes).  The
# default quota is unlimited.
#quota = inf

# The "wait" command below makes Wget wait between every connection.
# If, instead, you want Wget to wait only between retries of failed
# downloads, set waitretry to maximum number of seconds to wait (Wget
# will use "linear backoff", waiting 1 second after the first failure
# on a file, 2 seconds after the second failure, etc. up to this max).
waitretry = 10

# You can lower (or raise) the default number of retries when
# downloading a file (default is 20).
#tries = 20

     ]]>
    </content>
   </file>
  </section>
 </split>
  
 <split>
  <section>
   <heading>
Übersicht der wichtigsten Optionen
   </heading>

   <table>
    <pdf-column width="50"/>
    <pdf-column/>
    <tr>
     <td><command>-V</command></td>
     <td><command>--version</command></td>
     <td>Gibt die Version an</td>
    </tr>
    <tr>
     <td><command>-h</command></td>
     <td><command>--help</command></td>
     <td>Zeigt die Hilfe an</td>
    </tr>
    <tr>
     <td><command>-c</command></td>
     <td><command>--continue</command></td>
     <td>Nimmt den Download wieder auf</td>
    </tr>
    <tr>
     <td><command>-N</command></td>
     <td><command>--timestamping</command></td>
     <td>Besorgt nur neuere Dateien, als lokal vorhanden sind</td>
    </tr>
    <tr>
     <td><command>-r</command></td>
     <td><command>--recursive</command></td>
     <td>Lädt Dateien rekursiv herunter</td>
    </tr>
    <tr>
     <td><command>-o</command></td>
     <td><command>--output-file=datei</command></td>
     <td>Speicher die Ausgabe in datei</td>
    </tr>
    <tr>
     <td><command>-i</command></td>
     <td><command>--input-file=datei</command></td>
     <td>Liest URLs aus datei</td>
    </tr>
    <tr>  
     <td><command>-q</command></td>
     <td><command>--quiet</command></td>
     <td>Unterdrückt die Ausgabe</td>
    </tr>
    <tr>
     <td><command>-v</command></td>
     <td><command>--verbose</command></td>
     <td>Zeigt ausführlicher an, was wget macht</td>
    </tr>
    <tr>
     <td><command>-Y on/off</command></td>
     <td><command>--proxy=on/off</command></td>
     <td>Proxy ein- oder ausschalten</td>
    </tr>
    <tr>
     <td><command>-Q2m</command></td>
     <td><command>--quota=2m</command></td>
     <td>Beschränkt den Download auf 2 MB</td>
    </tr>
    <tr>
     <td><command>-nd</command></td>
     <td><command>--no-directories</command></td>
     <td>Erstellt keine Verzeichnisse</td>
    </tr>
    <tr>
     <td><command>-nH</command></td>
     <td><command>--no-host-directories</command></td>
     <td>Erstellt Ordner, lässt aber das Hostverzeichnis weg</td>
    </tr>
   </table> 	

   <textblock>
Für die ganze Liste siehe <command>man wget</command>
   </textblock>
  </section>

  <section>
   <heading>	   
Grafische Frontends
   </heading>
 
   <textblock>
Bei all seinen Optionen ist <command>wget</command> ein idealer 
Kandidat für ein Frontend. Mit <command>gtm</command> und 
<command>kwebget</command> gibt es zwei bekanntere Programme. 
Da die Bedienung mit grundlegenden Kenntnissen von <command>wget
</command> problemlos möglich ist, wird hier auf eine detaillierte 
Einführung verzichtet. Für programmspezifische Infos schaut man sich
am besten die den Tools beigelegte Hilfe an.
   </textblock>

   <ul>
    <li>
gtm: <ref lang="en" url="http://gtm.sourceforge.net/">
http://gtm.sourceforge.net/</ref>
    </li>
    <li>
kwebget: <ref lang="de" url="http://www.kpage.de/de/">
http://www.kpage.de/de/</ref>
    </li>
   </ul> 
  </section>
 </split>
</chapter> 
