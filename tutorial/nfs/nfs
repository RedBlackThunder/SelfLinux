<?xml version="1.0" encoding="iso-8859-1"?>
<chapter>
 <title>NFS - Network File System</title>

 <author>
  <name>Florian Frank</name>
  <mailto>florian.frank@pingos.org</mailto>
 </author>

 <author>
  <name>Katja Socher</name>
  <mailto>katja@linuxfocus.org</mailto>
 </author>

 <author>
  <name>Frédéric Raynal</name>
  <mailto>pappy@users.sourceforge.net</mailto>
 </author>

 <layout>
  <name>Florian Frank</name>
  <mailto>florian.frank@pingos.org</mailto>
 </layout>

 <license>GFDL</license>

 <index>nfs</index>

 <description>
  <textblock>
Ein Network File System (NFS) erlaubt das Verwalten von Dateien auf
mehreren Computern innerhalb eines Netzwerkes so, als wären sie auf
der lokalen Festplatte gespeichert. Dadurch ist es nicht nötig zu
wissen, wo die Dateien physikalisch gespeichert sind, um auf sie
zuzugreifen.
  </textblock>
 </description>

 <split>
  <section>
   <heading>
Einführung
   </heading>

   <textblock>
<strong>NFS</strong> erlaubt auf einfache Weise, Daten zwischen mehreren Computern zu
teilen. Zum Beispiel muss sich ein Benutzer, der in einem Netzwerk
angemeldet ist, nicht auch noch auf einem speziellen Computer
anmelden;
Über <strong>NFS</strong> hat er Zugriff auf sein <path>home</path>-Verzeichnis (man sagt,
es ist <strong>exportiert</strong>) auf der Maschine, auf der er gerade arbeitet.
   </textblock>

   <textblock>
NFS ist <strong>kein</strong> sehr effizientes Protokoll und deshalb sehr langsam,
wenn man es z. B. über eine Modemverbindung benutzt.
Es wurde für ein lokales Netzwerk entwickelt und
ist sehr flexibel. Es bietet eine Menge Möglichkeiten für Benutzer
und Administratoren.
   </textblock>

   <textblock>
Man muss diesen Dienst allerdings mit Vorsicht verwalten. Jedem zu
erlauben, Daten im Netz zu schreiben, wäre keine gute
Herangehensweise. Einige essentielle Massnahmen reduzieren dieses
Risiko. Sie werden später im Kapitel <ref iref="Potenzielle Sicherheitsprobleme">Potenzielle Sicherheitsprobleme</ref> beschrieben.
   </textblock>
  </section>

  <section>
   <heading>
Allgemeine Darstellung von Dateisystemen
   </heading>

   <textblock>
Bevor NFS beschrieben wird, sollte man den Ausdruck <strong>Dateisystem</strong>
verstehen. Ein Dateisystem ist ein Weg, um Daten auf einem
Medium zu speichern, der Weg, wie es organisiert und verwaltet
wird. Es gibt viele davon, einige sind weit verbreitet, andere
sind eher selten anzutreffen. Bekannte Dateisysteme sind etwa
das New Technology FileSystem (<ref lang="de" url="http://de.wikipedia.org/wiki/NTFS">NTFS</ref> von <ref chapter="win_vs_linux">Windows</ref>),
High Performance FileSystem (HPFS, OS/2), FAT 12/16/32, VFAT,
Macintosh Hierarchical Filesystem (HFS), ISO 9660 (für CD-ROM),
extended file systems (Ext, Ext2, Ext3), und viele andere.
   </textblock>

   <textblock>
Man kann jedes physikalische Medium für Daten (z. B. eine
Festplatte) als Feld von kleinen Einheiten betrachten, die
Informationen speichern: Man redet hier über <strong>Blöcke</strong>. Jedes Dateisystem
verwaltet diese Blöcke auf verschiedene Weise. Zum Beispiel versuchen
wir in der folgenden Abbildung eine Datei einzufügen, die zwei Blöcke
benutzt. Auf der oberen Illustration wurde die Datei hinter den
letzten belegten Block gesetzt, wobei leere Stellen am Anfang
auftreten. Im unteren Teil des Bildes (ein anderes Dateisystem)
wurde es an die erste freie Stelle gesetzt. Diese Politik hat
Einfluss darauf, wie stark eine Platte fragmentiert wird. Einige
Dateisysteme vermeiden Fragmentierung automatisch, während
andere manuell defragmentiert werden müssen.
   </textblock>

   <image height="250">
    <filename>nfs-1.png</filename>
   </image>

   <textblock>
Das bekannteste Dateisystem unter Linux ist <strong>ext2fs</strong> (extended 2 file
system). Jede Datei wird durch einen sogenannten <strong>Inode</strong> dargestellt.
Inodes sind Datenstrukturen, die Informationen über die Objekte eines
Dateisystemes speichern, wie z. B. Dateien oder Verzeichnisse.
Verzeichnisse speichern die Dateiliste, und der Zugriff geschieht
durch Operationen wie Lesen und Schreiben auf besondere Dateien.
   </textblock>

   <textblock>
Die Aufgabe eines <strong>NFS Servers</strong> ist es, den Clients die Inodes zu geben,
auf die sie zugreifen möchten. Allerdings würde ein Client nur allein
mit den Datei-Inodes nicht sehr gut arbeiten. Ein NFS Server ist eine
zusätzliche Netzschicht, die es remote-Rechnern erlaubt, Inodes zu
handhaben.
   </textblock>
  </section>
 </split>

 <split>

  <section>
   <heading>
Das NFS Protokoll
   </heading>

   <textblock>
Was man gewöhnlich <strong>NFS</strong> nennt, setzt sich aus vier verschiedenen
Protokollen zusammen. Jedes hängt von <strong>Remote Procedure Calls</strong> (RPC) und
<command>portmap</command> (auch <command>rpc.portmap</command> genannt) ab. Ein portmapper wandelt RPC
Programmnummern in Portnummern um. Wenn ein RPC Server startet, teilt
er <command>portmap</command> mit, welchen Port er benutzen wird, und die verwaltete RPC
Programmnummer. Wenn ein Client eine RPC Abfrage an eine gegebene
Programmnummer senden möchte, kontaktiert er zuerst den Server
<command>portmap</command>, um die Portnummer zu bekommen, die ihm Zugang zu dem
gewünschten Programm gibt. Dann adressiert er die RPC Pakete an den
korrespondierenden Port.
   </textblock>

   <textblock>
Die vier Dienste, die es NFS erlauben zu arbeiten, sind:
   </textblock>

   <section>
    <heading>
Network File System (nfs)
    </heading>

    <textblock>
Dieses Protokoll ist die Basis und erlaubt das Erzeugen von Dateien,
Suchen, Lesen oder Schreiben. Dieses Protokoll verwaltet auch die
Autentifizierung und die Dateistatistiken.
    </textblock>

    <textblock>
Der Daemon dieses Dienstes heisst <command>nfsd</command>.
    </textblock>
   </section>

   <section>
    <heading>
Mount Daemon (mountd)
    </heading>

    <textblock>
Dieser ist verantwortlich für das Mounten exportierter Systeme,
um auf sie mit NFS zugreifen zu können. Der Server empfängt Anfragen
wie <command>mount</command> und <command>umount</command> und muss auf diese Weise Informationen über
exportierte Dateisysteme bewahren.
    </textblock>

    <textblock>
Der Daemon dieses Dienstes heisst <command>mountd</command>.
    </textblock>
   </section>

   <section>
    <heading>
Network Status Monitor (nsm)
    </heading>

    <textblock>
Es wird benutzt, um Netzwerknodes zu überwachen, um den Zustand
einer Maschine (Client oder Server) zu kennen. Es informiert,
z.B. über einen Neustart.
    </textblock>

    <textblock>
Der Daemon dieses Dienstes heisst <command>statd</command>.
    </textblock>
   </section>

   <section>
    <heading>
Network Lock Manager (nlm)
    </heading>

    <textblock>
Um Datenänderungen durch mehrere Clienten zur gleichen Zeit zu
vermeiden, verwaltet dieses Protokoll ein <strong>Lock-System</strong>. Es
weiss, welche Dateien benutzt werden. Auf diese Weise ist es mit Hilfe
des Nsm Protokolls möglich zu wissen, wann ein Client erneut startet.
Nsm befreit alle Locks des Clienten, bevor sie zurückgegeben werden.
    </textblock>

    <textblock>
Der Daemon dieses Dienstes heisst <command>lockd</command>.
    </textblock>
   </section>

   <section>
    <heading>
Kernel NFS Daemon (knfsd)
    </heading>

    <textblock>
Der Daemon <command>knfsd</command>, verfügbar ab <strong>Kernelversion 2.4</strong>, unterstützt
direkt die nfs- und nlm-Protokolle. Andererseits werden <command>mountd</command> und
<command>nsm</command> noch nicht unterstützt. Wenn der NFS Server installiert
und gestartet wird, kann man mit dem folgenden Befehl verifizieren,
dass alles läuft:
    </textblock>

    <shell>
     <root>
ps auxwww | egrep "nfs|mount|lock|stat"
     </root>
     <output>
root       220  0.0  0.0  1456  524 ?        S    Feb21   0:00 /sbin/rpc.statd
root      3944  0.0  0.0     0    0 ?        SW   Feb21   3:33 [nfsd]
root      3945  0.0  0.0     0    0 ?        SW   Feb21   0:00 [lockd]
root      3947  0.0  0.0     0    0 ?        SW   Feb21   3:42 [nfsd]
root      3948  0.0  0.0     0    0 ?        SW   Feb21   3:17 [nfsd]
root      3949  0.0  0.0     0    0 ?        SW   Feb21   3:00 [nfsd]
root      3950  0.0  0.0     0    0 ?        SW   Feb21   3:38 [nfsd]
root      3951  0.0  0.0     0    0 ?        SW   Feb21   3:28 [nfsd]
root      3952  0.0  0.0     0    0 ?        SW   Feb21   3:35 [nfsd]
root      3953  0.0  0.0     0    0 ?        SW   Feb21   4:32 [nfsd]
root      3956  0.0  0.1  1612  824 ?        S    Feb21   0:02 /usr/sbin/rpc.mountd
root     21528  0.0  0.0  1356  500 pts/0    S    12:58   0:00 egrep nfs|mount|lock|stat
     </output>
    </shell>

    <textblock>
Momentan sind zwei NFS-Versionen verfügbar (Versionen 2 und 3 - sie
werden <strong>NFSv2</strong> bzw. <strong>NFSv3</strong> genannt, um sie zu unterscheiden). <strong>NFS4</strong>
befindet sich noch in der Entwicklungsphase, und ist im <ref chapter="kernel">Kernel</ref> als
<strong>experimentell</strong> gekennzeichnet.
    </textblock>

    <textblock>
Bei NFS geht es um eine Datenstruktur, die <strong>file handle</strong> genannt wird.
Dies ist eine Bitserie, die auf eindeutige Weise erlaubt, jedes
Dateisystemobjekt (wie eine Datei, aber nicht nur Dateien) zu
identifizieren. Sie enthält z.B. den Datei-Inode, aber auch einen
Eintrag, der das Gerät, auf dem sich die Datei sich befindet, enthält.
Daher können wir NFS als ein Dateisystem betrachten, das in ein
Dateisystem eingebettet ist.
    </textblock>
   </section>
  </section>
 </split>

 <split>
  <section>
   <heading>
Installation
   </heading>

   <section>
    <heading>
Der Server
    </heading>

    <textblock>
Das erste, das man tun muss, ist <command>portmap</command> zu starten, da dieses
Protokoll von NFS benötigt wird.
    </textblock>

    <shell>
     <root>
/usr/bin/rpcinfo -p
     </root>
     <output>
rpcinfo: can't contact portmapper: RPC: Remote system error - Connection refused
     </output>
    </shell>

    <shell>
     <root>
/sbin/portmap
     </root>
    </shell>

    <textblock>
Oder natürlich über das entsprechende Startskript der Distribution:
    </textblock>

    <shell>
     <root>
/etc/init.d/portmap start
     </root>
    </shell>

    <shell>
     <root>
/usr/bin/rpcinfo -p
     </root>
     <output>
       program vers proto   port
        100000    2   tcp    111  portmapper
        100000    2   udp    111  portmapper
     </output>
    </shell>

    <textblock>
Der Befehl <command>rpcinfo</command> zeigt die auf der Maschine laufenden RPC-Dienste,
spezifiziert als das Argument (<command>-p</command> Option). Im ersten Fall sieht man,
dass <command>portmap</command> noch nicht läuft: Man startet es (die meisten
Linux-Distributionen enthalten Skripte, um dies beim Starten zu
automatisieren). Danach wird erneut geprüft, ob es läuft. Ein anderer
oft vorkommender Grund für eine negative Antwort auf <command>rpcinfo</command> ist, dass
der portmapper nicht antworten darf aufgrund von
Sicherheitsrestriktionen in den Dateien <path>/etc/hosts.allow</path> oder
<path>/etc/hosts.deny</path>. In diesem Fall fügt man einen Eintrag zu der Datei
<path>/etc/hosts.allow</path> hinzu.
    </textblock>

    <file>
     <title>
/etc/hosts.allow
     </title>
     <content>
portmap: &lt;hosts&gt;
     </content>
    </file>

    <textblock>
Vor dem Starten von NFS selbst muss es konfiguriert werden. Es gibt
nur eine einzige Konfigurationsdatei, und die heisst <path>/etc/exports</path>. Jede
Zeile zeigt einen Verzeichnisbaum, der exportiert werden soll, gefolgt
von einer Liste von Clients, die auf ihn zugreifen dürfen. Es ist
möglich, am Ende jedes Client-Namens Optionen hinzuzufügen. Die
Manpage zu <command>exports</command> erklärt die Syntax für Client-Namen und Optionen.
    </textblock>

    <textblock>
Die akzeptierten Formen für Client-Namen sind:
    </textblock>

    <ul>
     <li>
Rechnername
     </li>
     <li>
Wildcards auf einen Domainnamen (z.B. : linux-*.mondomaine.fr)
     </li>
     <li>
eine netgroup (Netzgruppe) (@group), wenn <ref chapter="nis">NIS</ref> benutzt wird
     </li>
     <li>
eine <ref chapter="tcpip" iref="Adressierung im IP">IP-Adresse</ref>
     </li>
    </ul>

    <textblock>
Ohne im Detail alle verfügbaren <command>mount</command>-Optionen beschreiben zu wollen,
hier dennoch eine Aufzählung der wichtigsten Optionen:
    </textblock>

    <ul>
     <li>
<command>rw</command> (read write): Der Client kann im exportierten Sytem lesen und schreiben
     </li>
     <li>
<command>ro</command> (read only): Der Client kann im exportierten System nur lesen
     </li>
     <li>
<command>root_squash</command>: Es ist sicherer, wenn der <ref chapter="nutzer_unter_linux" iref="root">root-Benutzer</ref> eines
Client <strong>nicht</strong> mit Root-Erlaubnis schreiben darf. Um dies zu vermeiden,
wird die <ref chapter="nutzer_unter_linux" iref="Benutzer-ID (UID) und Gruppen-ID (GID)">UID/GID</ref> 0 (z. B. die von <ref chapter="nutzer_unter_linux" iref="root">root</ref>) auf der Seite des Clients in
den Benutzer <strong>nobody</strong> übersetzt. Diese Option ist standartmässig aktiv,
kann aber mit <command>no_root_squash</command> abgeschaltet werden.
     </li>
     <li>
<command>all_squash</command>: Alle Clients, die auf das oben exportierte System
zugreifen, benutzen die <ref chapter="nutzer_unter_linux" iref="Benutzer-ID (UID) und Gruppen-ID (GID)">UID/GID</ref> des Benutzers <strong>nobody</strong>
     </li>
     <li>
<command>anonuid</command>, <command>anongid</command>: Der Benutzer <strong>nobody</strong> benutzt jetzt <ref chapter="nutzer_unter_linux" iref="Benutzer-ID (UID) und Gruppen-ID (GID)">UID und GID</ref>
definiert durch diese Optionen.
     </li>
    </ul>

    <textblock>
Jetzt müssen die <command>rpc.mountd</command> und <command>rpc.nfs</command> Daemons gestartet werden, um
einen laufenden NFS-Server zu bekommen. Man sollte nochmals mit dem
Befehl <command>rpcinfo</command> überprüfen, dass alles läuft. Man kann den Server sogar
für die nsm- und nlm-Protokolle initialisieren (<command>rpc.statd</command> bzw.
<command>rpc.lockd</command>). Sie sind aber nicht unbedingt notwendig, um einen
NFS-Server laufen zu lassen. Allerdings sind sie hilfreich, wenn eine
Maschine ausfällt, von selbst rebootet, etc...
    </textblock>

    <textblock>
Wenn man die Konfigurationsdatei <path>/etc/exports</path> ändert, muss man die
betroffenen Daemons informieren, dass Änderungen gemacht wurden. Der
Befehl <command>exportfs</command> übermittelt diese Information an die Server. Die
Option <command>-r</command> synchronisiert die Datei <path>/var/lib/nfs/etab</path> mit der Datei
<path>/etc/exports</path>. Die Option <command>-v</command> zeigt die exportierten Dateisysteme
zusammen mit ihren Optionen an.
    </textblock>

    <textblock>
Nach dem Start enthalten die folgenden Dateien wichtige Informationen:
    </textblock>

    <ul>
     <li>
<path>/var/lib/nfs/rmtab</path>: Jede Zeile zeigt den Namen des Clients und
das Dateisystem, das von diesem Server importiert wurde.
     </li>
     <li>
<path>/var/lib/nfs/etab</path>: Die Datei <path>/etc/exports</path> enthält nur eine
Wunschliste. <path>etab</path> wird durch <command>exportfs</command> erzeugt. Es enthält auf jeder
Zeile detaillierte Informationen über die Optionen, die benutzt
werden, wenn ein Dateisystem zu einem einzelnen Client exportiert wird.
Es ist die Referenzdatei, die von <command>rpc.mountd</command> benutzt wird, wenn dieser
gestartet wird.
     </li>
     <li>
<path>/proc/fs/nfs/exports</path>: enthält die Client-Liste, so wie sie der
<ref chapter="kernel">Kernel</ref> kennt.
     </li>
     <li>
<path>/var/lib/nfs/xtab</path>: Wird benutzt für die Genauigkeit, wenn <path>etab</path>
Clientennamen und Rechnergruppen mit <strong>Wildcards</strong> enthält. Diese Datei
enthält nur explizite Rechnernamen.
     </li>
    </ul>

    <textblock>
Wenn ein Client auf ein Dateisystem zugreifen möchte, fängt er damit
an, <command>mountd</command> zu fragen. Dieser sucht dann in <path>etab</path>, ob die Abfrage
beantwortet werden kann. Er prüft ebenso, ob der Client zu dieser
Abfrage berechtigt ist (<path>hosts.{allow, deny}</path>, <ref chapter="iptables">Regeln der Firewall</ref>, ...).
Der <ref chapter="kernel">Kernel</ref> benutzt <command>exportfs</command> für die Überprüfung. Wenn in der
Datei <path>/var/lib/nfs/etab</path> das exportierte System berechtigt ist, zu der
Gruppe, zu der der Client gehört, exportiert zu werden, dann informiert
<command>mountd</command> den <ref chapter="kernel">Kernel</ref>, der daraufhin <path>xtab</path> mit dem neuen Rechnernamen aktualisiert.
    </textblock>
   </section>

   <section>
    <heading>
Der Client
    </heading>

    <textblock>
Hier gibt es normalerweise wenig zu tun. Der Zugriff auf ein
Dateisystem, das von NFS exportiert wurde, wird direkt vom <ref chapter="kernel">Kernel</ref>
verwaltet. Er muss mit <ref chapter="kernel">Unterstützung für NFS</ref> kompiliert worden sein.
Die Datei <path>/proc/filesystems</path> listet alle Dateisysteme auf, die direkt
vom <ref chapter="kernel">Kernel</ref> unterstützt werden. Man muss dann nur dem <ref chapter="kernel">Kernel</ref> sagen,
dass man Zugriff auf ein von NFS exportiertes System haben möchte.
    </textblock>

    <textblock>
Der Befehl <command>mount</command> berechtigt zum Zugriff auf verschiedene Dateisysteme.
Er informiert den <ref chapter="kernel">Kernel</ref>, dass ein neues Dateisystem verfügbar ist,
mit Angabe seines Typs, des Gerätes und seinem Mount-Punkt. Die Option
<command>-t</command> kann dazu benutzt werden, um den Typ des benutzten Dateisystems zu
spezifizieren. Für NFS schreibt man <command>-t nfs</command>.
    </textblock>

    <textblock>
<command>mount</command> hat seine eigenen Optionen für NFS. Zum Beispiel können die
Optionen <command>rsize</command> und <command>wsize</command> dazu benutzt werden, um die Blockgrößen für
Lesen und Schreiben zu ändern. Man kann NFS-spezifische Optionen mit
allgemeineren Optionen wie <command>intr</command>, <command>noexec</command> oder <command>nosuid</command> verbinden. Die
<ref chapter="linux_hilfe" iref="man">manpage</ref> zu <command>mount</command> listet alle diese Optionen auf.
    </textblock>

    <textblock>
Wir nehmen an, der Rechner <strong>enterprise</strong> ist ein NFS-Server und
exportiert sein Verzeichnis <path>/usr/local</path>. Wenn man darauf vom Rechner
<strong>excelsior</strong> aus zugreifen will, dann muss man nur das exportierte
Verzeichnis von <strong>enterprise</strong> auf <strong>excelsior</strong> mounten:
    </textblock>

    <shell>
     <root>
mount -t nfs -o nosuid,hard,intr enterprise:/usr/local /usr/local
     </root>
    </shell>

    <textblock>
Der Befehl zeigt an, dass man ein NFS Dateisystem mountet (<command>-t nfs</command>),
mit den Optionen <command>nosuid</command>, <command>hard</command> und <command>intr</command>. Die beiden letzten Argumente
sind die interessantesten. Das erste der beiden spezifiziert das zu
mountende Gerät. Für NFS ist die Syntax anders als in der gewöhnlichen
Befehlszeile von <command>mount</command>, wo man das Gerät und das Verzeichnis spezifiziert.
Hier spezifiziert man <strong>server:exported_directory</strong> anstelle eines
Gerätes.
    </textblock>

    <textblock>
Das letzte Argument zeigt die Stelle des Dateisystems auf der
Client-Seite an. Hier teilen sich <strong>excelsior</strong> und <strong>enterprise</strong> einfach
das Verzeichnis <path>/usr/local</path>. Dadurch kann vermieden werden, Programme
mehr als einmal in <path>/usr/local</path> zu installieren. Um dies permanent zu
machen, kann man dies auf <strong>excelsior</strong> in der Datei <path>/etc/fstab</path>
spezifizieren. <path>fstab</path> enthält alle Dateisysteme, die beim Start
gemounted werden müssen. Die Syntax für <path>/etc/fstab</path> lautet:
    </textblock>

    <file>
     <title>
/etc/fstab
     </title>
     <content>
#    device           mount point file system  options          dump fsckorder
enterprise:/usr/local /usr/local  nfs          nosuid,hard,intr 0    0
     </content>
    </file>

    <textblock>
Man sollte jedoch mit permanenten Einträgen vorsichtig sein. Man kann
es nur benutzen, wenn der Server (<strong>enterprise</strong>) immer eingeschaltet ist
oder vor <strong>excelsior</strong> eingeschaltet wird.
    </textblock>
   </section>
  </section>
 </split>

 <split>
  <section>
   <heading>
Potenzielle Sicherheitsprobleme
   </heading>

   <textblock>
Ein grosses Problem mit NFS entsteht aufgrund der Tatsache, dass
standardmässig ein <strong>vertrauensvolles Verhältnis</strong> zwischen Client und
NFS-Server besteht. In dem Fall, dass der <ref chapter="nutzer_unter_linux" iref="root">root-Account</ref> des Servers
kompromittiert wird, wird auch der des Clienten kompromittiert.
   </textblock>

   <textblock>
Ein Client darf nicht blind einem Server trauen und umgekehrt, deshalb
muss man einschränkende Optionen spezifizieren, wenn der Befehl <command>mount</command>
benutzt wird. Die erste wurde bereits erwähnt: <command>nosuid</command>. Es macht den
Effekt des <strong>SUID</strong> und <strong>SGID</strong> Bits rückgängig. Das heisst, eine als <ref chapter="nutzer_unter_linux" iref="root">root</ref>
angemeldetet Person auf dem Server muss sich zuerst als Benutzer auf
dem Client anmelden und wird erst dann <ref chapter="nutzer_unter_linux" iref="root">root</ref>. Eine andere, restriktivere
Option ist <command>noexec</command>. Diese verbietet das Ausführen von Programmen auf
exportierten Dateisystemen. Diese Option ist nur auf Systemen
sinnvoll, die nur Daten enthalten.
   </textblock>

   <textblock>
Auf der NFS-Serverseite kann man ebenso spezifizieren, dass dem
<ref chapter="nutzer_unter_linux" iref="root">root-Account</ref> des Client nicht vertraut werden soll. Man muss dies
in <path>/etc/exports</path> mit der Option <command>root_squash</command> spezifizieren. Wenn ein
Benutzer mit <ref chapter="nutzer_unter_linux" iref="Benutzer-ID (UID) und Gruppen-ID (GID)">UID</ref> 0 (<ref chapter="nutzer_unter_linux" iref="root">root</ref>) auf dem Client auf das Dateisystem, das
vom Server exportiert wurde, zugreift, wird er für alle Abfragen zum
Benutzer <strong>nobody</strong> umgewandelt. Diese Option ist standardmäßig unter
Linux aktiv, kann aber mit der Option <command>no_root_squash</command> abgeschaltet
werden. Eine Menge von <ref chapter="nutzer_unter_linux" iref="Benutzer-ID (UID) und Gruppen-ID (GID)">UIDs</ref> kann spezifiziert werden, auf die diese
Option angewendet werden soll. Es ist auch möglich, mit den Optionen
<command>anonuid</command> und <command>anongid</command> Optionen den Zielbenutzer zu definieren, auf den
die Anfrage umgewandelt werden soll.
   </textblock>

   <textblock>
Einige Aktionen sind allgemeiner und haben Auswirkungen auf den
portmapper. Zum Beispiel kann man mit der folgenden Zeile in der Datei
<path>/etc/hosts.deny</path> allen Rechner den Zugriff verbieten:
   </textblock>

   <file>
    <title>
/etc/hosts.deny
    </title>
    <content>
# hosts.deny :
#              use the portmap
portmap: ALL
    </content>
   </file>

   <textblock>
Die Datei <path>/etc/hosts.allow</path> stellt das Gegengewicht zu diesem strikten
Verbot dar und erlaubt den Zugriff von den eingetragenen Maschinen.
   </textblock>

   <textblock>
Gute <ref chapter="iptables">Firewall-Regeln</ref> tragen auch zu einem besseren Schutz bei. Es
werden verschiedene Ports von den unterschiedlichen Diensten genutzt:
   </textblock>

   <table>
    <pdf-column width="50"/>
    <pdf-column width="50"/>
    <pdf-column/>
    <tr>
     <th>
Service
     </th>
     <th>
Port
     </th>
     <th>
Protokoll(e)
     </th>
    </tr>
    <tr>
     <td>
portmap
     </td>
     <td>
111
     </td>
     <td>
upd / tcp
     </td>
    </tr>
    <tr>
     <td>
nfsd
     </td>
     <td>
2049
     </td>
     <td>
udp
     </td>
    </tr>
    <tr>
     <td>
mountd
     </td>
     <td>
variabel
     </td>
     <td>
udp / tcp
     </td>
    </tr>
   </table>

   <textblock>
Weitere Informationen zur Sicherheit sind im Kapitel
<ref chapter="grundlagen_sicherheit">Grundlagen Sicherheit</ref>
und <ref chapter="iptables">Iptables</ref> zu finden.
   </textblock>
  </section>
 </split>
</chapter>
