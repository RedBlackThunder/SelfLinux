<?xml version="1.0" encoding="iso-8859-1"?>

<chapter>
 <title>Computergeschichte</title>

 <author>
  <name>Matthias Kleine</name>
  <mailto>kleine_matthias@gmx.de</mailto>
 </author>

 <layout>
  <name>Matthias Kleine</name>
  <mailto>kleine_matthias@gmx.de</mailto>
 </layout>

 <license>
  GPL
 </license>

 <index>computer_geschichte</index>

 <description>
  <textblock>
In diesem Kapitel wird die Geschichte des Computers von seinen
mechanischen Anfängen, über die frühen Relais- und Röhrenrechner
bis hin zu den auf Microchips basierenden Rechnern der Neuzeit
behandelt. Auch wenn dieses Kapitel nicht in unmittelbarem
Zusammenhang mit Linux steht, so bietet es doch interessante
Hintergrundinformationen, welche die Möglichkeiten und
Einschränkungen beim Umgang mit einem Computer, also auch
einem Linuxsystem, verständlicher machen können.
  </textblock>
 </description>

 <split>
  <section>
   <heading>
Analoges
   </heading>

   <textblock>
Man kann die Geschichte der Computer erst mit dem Aufkommen digitaler
Rechengeräte beginnen lassen, aber wer sich einmal mit analogen
Rechengeräten beschäftigt hat, wird dies sicher schade finden. Selbst
die verschiedensten historischen Mess- und Zeichengeräte möchte man
als Vorläufer des heutigen universalen Computers betrachten. Zirkel,
Lineale, Rädertriebe u.ä. wurden bereits in der Antike verwendet und
leisteten für ihre Verhältnisse unschätzbare Dienste in Geometrie,
Landvermessung, Navigation und Astronomie - sicher alles Tätigkeitsfelder,
in denen heute der Computer Einzug gehalten hat.
   </textblock>

   <textblock>
Um sich bei der Berechnung großer Zahlen zu behelfen, hat der Mensch
zahlreiche Hilfsmittel erdacht, die mitunter skurril anmuten, aber
dennoch teilweise erstaunlich leistungsfähig waren. Der mittelalterliche
Abakus lässt mit Leichtigkeit Berechnungen bis in den Milliardenbereich zu.
Noch heute lernen z.B. in China die Absolventen kaufmännischer Berufe
mit einem ganz ähnlichen Gerät zu rechnen.
   </textblock>

   <textblock>
Eine ganz eigene Welt öffnet sich mit dem Aufkommen mechanischer
Addiermaschinen. Basierten die ersten dieser Geräte noch auf handgefeilten
Zahnrädern und produzierten somit noch gewisse Ungenauigkeiten, leisteten
spätere Varianten mit "verbesserter Hardware" doch schon Erstaunliches.
Insbesondere das Problem des Zehnerübertrags lässt die Walzen und Hebel
einer solchen Maschine mitunter phantastische Prozeduren durchlaufen.
In heutigen Rechenchips hat sich dieses Problem in Form von
"Übertragsbits" erhalten - die Zeiten ändern sich, aber die Probleme
bleiben ...
   </textblock>

   <textblock>
Ein interessantes Gebiet, das heute zu den wichtigsten in der Informatik
gehört, ist die <ref chapter="gpg_handbuch_vorwort">Verschlüsselung</ref> von Information, bevorzugt Textinformation.
Frühe Algorithmen, die vorwiegend auf Substitution basieren, bieten bereits
einige über 3000 Jahre alte Geheimschriften der Ägypter und Mesopotamier
an. Später entwickelten sich dynamischere Zeichenzuordnungen, indem z.B.
verstellbare Ringe zur Chiffrierung und Dechiffrierung verwendet wurden.
In der Neuzeit gewinnen Chiffriergeräte insbesondere im militärischen
Bereich an Bedeutung. Hier sei etwa an die Geschichte der ENIGMA
erinnert - ursprünglich gebaut aus zwei elektrischen Schreibmaschinen, die
durch ein Bündel von Drähten verbunden wurden. Eine Weiterentwicklung dieses
Geräts wurde im zweiten Weltkrieg von der deutschen Wehrmacht verwendet - über
100.000 ENIGMAs ließ sie bauen und vertraute nahezu blind auf die Sicherheit
ihrer Verschlüsselung. Die Tatsache, dass die Briten die Verschlüsselung der
ENIGMA durch den Einsatz tausender Mathematiker brechen konnten, spielte
keine unerhebliche Rolle für den weiteren Verlauf der Ereignisse.
   </textblock>
  </section>
 </split>

 <split>
  <section>
   <heading>
Der Einbruch des Digitalen
   </heading>

   <textblock>
Alle bislang erwähnten Hilfsmittel und Geräte haben nichts mit dem
Dualsystem zu tun und sind daher nur indirekte Vorläufer heutiger Rechner.
Erst die Verknüpfung des Dualsystems mit einigen technischen Entwicklungen
ließ die Leistung der entwickelten Rechengeräte Schlag um Schlag wachsen.
   </textblock>

   <textblock>
Als Erster beschrieb <ref lang="de" url="http://de.wikipedia.org/wiki/Gottfried_Wilhelm_Leibniz">Gottfried Wilhelm Leibniz</ref> eine Rechenmaschine zur
Durchführung von Rechnungen im Dualsystem. Viel später, im Jahr 1933,
entschloss sich <ref lang="de" url="http://de.wikipedia.org/wiki/Konrad_Zuse">Konrad Zuse</ref>, das Dualsystem seiner geplanten Rechenmaschine
zu Grunde zulegen. Dieses Gerät, die sogenannte Z1, wurde 1939 fertiggestellt
und verwendete rein mechanische Schalt- und Speicherglieder.
   </textblock>

   <textblock>
Die Geschwindigkeit (und die Menge) mechanischer Glieder ist naturgemäß
beschränkt. Der Fortschritt auf dem Gebiet der Elektrotechnik ermöglichte
nun den Einsatz von elektromechanischen Relais anstelle der rein mechanischen
Teile. Ein Schaltkreis dient auf einfachste Weise zur Abbildung des dualen
Systems: Durch Schaltkontakte kann der Stromfluss geöffnet und geschlossen
werden. AUS-Zustand und EIN-Zustand können nun die 0 und die 1 des Dualsystems
abbilden. Das Schließen eines solchen elektrischen Kontaktes
kann auf verschiedene Arten geschehen.
   </textblock>

   <textblock>
Die 1940/41 von Zuse erbaute Z3 verwendete elektromagnetische Relais,
in denen eine Spule ein Magnetfeld erzeugte und dadurch ein
Metallplättchen anzog. Dieses schloss den Stromkreis - das
Bit hatte seinen Wert geändert. Die Z3 in Aktion hat einen unvergleichlichen
Charme. Nach der Eingabe einer Rechenaufgabe beginnen hunderte von
Metallplättchen der aufgereihten Relais scheinbar unkoordiniert zu wippen und
zu klappern, bis das Gerät schließlich auf wundersame Art und
Weise verstummt und das Ergebnis der Berechnung präsentiert. Interessant
ist, dass CPU und Speicher gewissermaßen in ein linkes und ein rechtes
"Relais-Regal" unterteilt sind. Nach einer Berechnung kann man den
Speicherrelais ansehen, dass sie teilweise "EIN" oder "AUS" sind - auch wenn
man nicht im entferntesten erahnen kann, warum es nun gerade diese Auswahl an
Plättchen ist, die auf "EIN" steht. Glücklicherweise werden Rechenergebnisse
benutzerfreundlich auf einem Bedienelement angezeigt - im Dezimalsystem
wohlgemerkt.
   </textblock>

   <textblock>
Der nächste rein elektrotechnische Fortschritt bestand in der Verwendung
von Röhrenschaltungen anstelle von Relais. Wir wollen hier nicht auf die
Technik von Elektronenröhren eingehen, aber ihr Vorteil gegenüber den teilweise
noch mechanischen Relais besteht in der viel höheren Geschwindigkeit, mit
der sie von EIN nach AUS oder umgekehrt umschalten können. Schaltzeiten von
einigen Mikrosekunden sind mit Röhren möglich. Das Grundprinzip aber bleibt
dasselbe: Geschlossener bzw. offener Stromkreis bilden 0 und 1 des dualen
Systems ab und ermöglichen somit Berechnung und Speicherung. Besonders leicht
lässt sich mit Hilfe von Röhren die logische Grundfunktion NOR realisieren.
Der erste aus Elektronenröhren aufgebaute
Rechner war der ENIAC (Electronic Numerical Integrator and Computer, USA
1946). Er hatte ca. 18.000 Röhren und benötigte fast 140 Kilowatt.
   </textblock>

   <textblock>
Röhren waren zwar schnell, aber sie gingen leicht kaputt. Um genau zu sein,
waren die Bedienteams der Röhrenrechner mehr damit beschäftigt, die
Röhrenbänke präventiv auszutauschen, als dass sie irgendwelche Programme
laufen lassen konnten. Außerdem sind Röhren so groß, dass die Röhrenrechner
das Ausmaß eines Kinderzimmers annahmen. Es dauerte nicht allzu lange, bis
Transistor-Schaltkreise an die Stelle von Röhrenschaltungen traten.
Transistoren sind kleiner, leben länger, verbrauchen weniger Strom, entwickeln
weniger Wärme und können dadurch auch dichter gepackt werden. Die Schaltzeiten
liegen im Nanosekundenbereich. Um 1960 waren die Röhren in Computern
nahezu vollständig durch Transistoren verdrängt. Bis heute bilden
Transistoren das Basiselement der Recheneinheiten eines Computers. In modernen
CPUs sind heute knapp 100 Millionen Transistoren verbaut.
   </textblock>

   <textblock>
Die weiteren Fortschritte auf elektrotechnischer Ebene wurden durch die
Miniaturisierung der Transistortechnik erzielt. Mit der Entdeckung von
Silizium als Baumaterial wurde es zunächst möglich, Dutzende von Transistoren
auf einen Chip zu packen. Diese Entwicklung setzte sich fort und ermöglichte
bald Tausende, Hunderttausende und schließlich Millionen von Transistoren auf
einem einzigen Chip. Diese Chips konnten massenhaft produziert werden und
ermöglichten so die Konstruktion von relativ günstigen Minicomputern. Da es
sich hier im Wesentlichen um physikalische und chemische Fortschritte bei
den Fertigungsprozessen handelt, wollen wir diese Entwicklung nicht im Detail
verfolgen. Wenden wir uns stattdessen der Entwicklung der Konzepte zu, die
zum heutigen, mit einem (oder gar mehreren) Betriebssystem(en) ausgestatteten,
programmierbaren Computer führte.
   </textblock>
  </section>
 </split>

 <split>
  <section>
   <heading>
Das von Neumann-Modell
   </heading>

   <textblock>
Ein Mitglied des ENIAC-Projektes war <ref lang="de" url="http://de.wikipedia.org/wiki/John_von_Neumann">John von Neumann</ref>. Von Neumann war ein
weltweit bekannter Mathematiker und wird gelegentlich als ein Genie vom
Range eines Leonardo da Vinci bezeichnet. Er hatte ein phänomenales Gedächtnis
und neben der Mathematik auch ausgezeichnete Kenntnisse in Physik. Ihm fiel
auf, dass die Programmierung des ENIAC über das Umstecken von Kabeln und das
Betätigen von Schaltern viel zu umständlich war. Er entwarf daher ein
Konzept, das heute als "von Neumann-Maschine" bekannt ist, und nach dem
auch das Programm selbst im Speicher des Rechners abgelegt wird. Man
muss nicht lange überlegen, um zu verstehen, dass diese Idee die
Programmierung von Computern revolutionierte. Wenn sich ein Programm im
Speicher des Rechners befindet, sind lediglich einige Ladevorgänge
erforderlich, um ein anderes Programm auf dem Rechner laufen zu lassen.
Damit wird die Maschine flexibler und universeller, und das
Schreiben und Testen von Programmen wird viel einfacher.
   </textblock>

   <image height="100">
    <title>von-Neuman Modell</title>
    <filename>neumann-modell.png</filename>
   </image>

   <textblock>
Das Modell besteht aus fünf Teilen: Speicher, Rechenwerk, Steuereinheit, sowie Ein- und
Ausgabe. Die wichtigsten Prinzipien lauten wie folgt:
   </textblock>

   <ul>
    <li>
Der Speicher enthält sowohl die Operationen selbst als auch die Daten,
auf denen die Operationen ausgeführt werden.
    </li>
    <li>
Die Steuereinheit entspricht einem Befehlsprozessor, in dem die Anweisungen
eines Programmes interpretiert und die Ausführung dieser Befehle (z.B. in
der richtigen Reihenfolge) gesteuert werden.
    </li>
    <li>
Das Rechenwerk (ALU = arithmetic logical unit) entspricht einem Datenprozessor,
in dem die notwendigen datentransformierenden Operationen durchgeführt werden.
    </li>
    <li>
Ein- und Ausgabe bilden die Schnittstelle zur Außenwelt, es ist jedoch
nicht festgelegt, wie diese Mechanismen implementiert sind.
    </li>
   </ul>
  </section>
 </split>

 <split>
  <section>
   <heading>
Die Entwicklung des Massenmarktes bis zum heutigen PC
   </heading>

   <textblock>
Der erste echte "Minicomputer" auf der Basis von Transistoren war die PDP 1
der Digital Equipment Corporation (DEC). Die PDP 1 kam 1961 auf den Markt.
Sie war zwar "nur" halb so schnell wie die damals schnellste Rechenmaschine,
die IBM 7090, kostete dafür aber nur einen Bruchteil der IBM, nämlich
$ 120.000. DEC verkaufte Dutzende PDP 1. Ein richtiger Renner hingegen
sollte einige Jahre später die PDP 8 werden, die nur noch $ 12.000 kostete
und über 50.000 Mal verkauft wurde. DEC war zur damaligen Zeit der Markführer
im Minicomputer-Geschäft.
   </textblock>

   <textblock>
Mit der PDP 8 wurde eine interessante Neuerung eingeführt. Es wurden
alle Komponenten der PDP 8 über einen einzigen Bus, den so genannten Omnibus,
miteinander verbunden. Die Kommunikation der CPU mit Speicher und Geräten
geschieht also über ein gemeinsames Bündel paralleler Drähte. CPU, Speicher und
E/A-Geräte müssen sich diesen Bus teilen und sich darüber abstimmen, wer wann
Daten über den Bus schicken darf. Die Idee eines solchen Busses finden wir auch
heute noch in gewöhnlichen PCs, z.B. als PCI-Bus oder auch als USB-Bus.
   </textblock>

   <textblock>
Etwa ab 1965 bot IBM eine neue Serie von Rechnern auf der Basis von
integrierten Schaltungen an, die sogenannte 360er Serie. Es handelte sich
dabei um eine ganze Familie von Rechnern, welche die gesamte Bandbreite
der Anforderungen vom wissenschaftlichen Supercomputer bis zum Einsatz im
kommerziellen Bereich abdecken sollte. Erstmals achtete IBM darauf, dass
Software, die für die kleineren Modelle geschrieben wurde, auch auf den
größeren laufen konnte und umgekehrt. Das Konzept der Rechnerfamilie war
geboren und sollte sich bald durchsetzen.
   </textblock>

   <textblock>
Die 360er Rechner wiesen eine weitere Neuerung auf, den sogenannten
Mehrprogrammbetrieb. Dabei können sich zu einem Zeitpunkt mehrere Programme
gleichzeitig im Speicher befinden. Diese Möglichkeit führte zu einer
besseren Ausnutzung der CPU, weil jeweils ein anderes Programm
ausgeführt werden konnte, wenn ein Programm in seinem Ablauf unterbrochen
wurde, um beispielsweise eine langsame Ein-/Ausgabe durchzuführen.
   </textblock>

   <textblock>
Trotz DECs Bemühungen, die Preise auf dem Minicomputer-Markt zu drücken
(mittlerweile war die PDP 11 insbesondere an Universitäten sehr erfolgreich),
blieben Computer bis ca. 1980 für Normalsterbliche unerschwinglich. Die
immer dichter mit Transistoren bepackten Chips führten unterdessen dazu,
dass Rechenleistung und Speicher immer erschwinglicher wurden. Damit begann die
Ära des Personal Computers (PC).
   </textblock>

   <textblock>
Lassen wir die wenigen Vorläufer unseres heutigen PCs (wie z.B. die frühen
Apple-Computer, die Commodores, Amigas und Ataris, deren ehemaligen Besitzern
noch heute ein Glänzen in die Augen steigt, wenn von ihnen die Rede ist)
sträflich außer acht und steuern direkt auf die Neuzeit zu. IBM hatte
diesen Markt eine Weile beobachtet und dann entschieden, selbst einzusteigen.
Da keine Zeit zu verlieren war, baute man den IBM-PC aus Teilen, die bereits
auf dem Markt erhältlich waren. 1981 führte IBM seine Konstruktion auf dem
Markt ein und verbuchte damit sofort einen überwältigenden Erfolg.
   </textblock>

   <textblock>
Nun tat IBM etwas, was im Nachhinein vielleicht als Fehler zu bezeichnen ist.
Um anderen Herstellern die Entwicklung von
Steckkarten für seinen PC zu ermöglichen, veröffentlichte es sämtliche
Pläne und Schaltdiagramme des IBM-PCs. Da alle Teile ohnehin auf dem Markt
erhältlich waren, fanden sich schnell alternative Anbieter, die Klone
des PCs herstellten. Damit war ein neuer und milliardenschwerer Markt
geboren.
   </textblock>
   <textblock>
Ungefähr um diese Zeit begann auch der Aufstieg eines kleinen Unternehmens
namens Microsoft. Diese Geschichte zu erzählen, wollen wir jedoch anderen
überlassen.
   </textblock>

   <textblock>
Da sich an der grundlegenden PC-Architektur seit der Einführung
des IBM-PCs nichts Wesentliches mehr geändert hat, sind wir bereits in
der Gegenwart angelangt und wollen unseren kurzen Streifzug durch die
Computergeschichte an dieser Stelle beenden. Wir tun dies in dem Bewusstsein,
viele großartige Meilensteine nicht einmal erwähnt zu haben.
   </textblock>
  </section>

  <section>
   <heading>
Mit eigenen Augen
   </heading>

   <textblock>
Wer sich für einen tieferen Einblick in die Geschichte der Datenverarbeitung
interessiert, dem sei das <ref lang="de" url="http://www.hnf.de">Heinz Nixdorf Museums-Forum</ref> in Paderborn empfohlen.
Die Ausstellung des Museums erzählt mit über 2000 Objekten die Geschichte
der Informationstechnik.
   </textblock>
  </section>
 </split>
</chapter>
